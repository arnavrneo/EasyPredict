# LightPredict

LightPredict is a library that aims to build the most commonly used machine learning models without much need to write the code.
<hr/>

# Installation

```
pip install lightpredict
```

# Usage

### Importing
```
import lazypredict
```

## Classification
Example

```
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from lightpredict import LightClassifier

data = load_breast_cancer()
X = data.data
y= data.target

X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=.5,random_state =123)
lcf = LightClassifier()
lcf.fit(X_train,X_test,y_train,y_test,rounds=5,plot=True)  # if plot=True, a plot will be generated comparing the accuracy of all models for easy comparison

╔═══════════════════════════════════════════════════════════════════════════════════════════╗
║                                   Classification Models                                   ║
╚═══════════════════════════════════════════════════════════════════════════════════════════╝
┏━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┓
┃       Model        ┃ Accuracy score ┃ f1-score ┃ ROC-AUC ┃ Precision score ┃ Recall score ┃
┡━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━┩
│ AdaBoostClassifier │     0.9614     │  0.9697  │ 0.9873  │     0.97238     │   0.96703    │
├────────────────────┼────────────────┼──────────┼─────────┼─────────────────┼──────────────┤
│ BaggingClassifier  │    0.96491     │ 0.97207  │ 0.9885  │     0.98864     │   0.95604    │
├────────────────────┼────────────────┼──────────┼─────────┼─────────────────┼──────────────┤
│    BernoulliNB     │     0.6386     │ 0.77944  │ 0.51648 │     0.6386      │     1.0      │
├────────────────────┼────────────────┼──────────┼─────────┼─────────────────┼──────────────┤
│    DecisionTree    │    0.93333     │ 0.94766  │ 0.92884 │     0.95028     │   0.94505    │
├────────────────────┼────────────────┼──────────┼─────────┼─────────────────┼──────────────┤
│     ExtraTree      │    0.91579     │ 0.93407  │ 0.90878 │     0.93407     │   0.93407    │
├────────────────────┼────────────────┼──────────┼─────────┼─────────────────┼──────────────┤
│     GaussianNB     │    0.95789     │ 0.96739  │ 0.99323 │     0.95699     │   0.97802    │
├────────────────────┼────────────────┼──────────┼─────────┼─────────────────┼──────────────┤
│     KNeighbors     │    0.93684     │ 0.95135  │ 0.96242 │     0.93617     │   0.96703    │
├────────────────────┼────────────────┼──────────┼─────────┼─────────────────┼──────────────┤
│     LinearSVC      │    0.94035     │ 0.95491  │   N/A   │     0.92308     │   0.98901    │
├────────────────────┼────────────────┼──────────┼─────────┼─────────────────┼──────────────┤
│    LogisticReg     │     0.9614     │ 0.97003  │ 0.98389 │     0.96216     │   0.97802    │
├────────────────────┼────────────────┼──────────┼─────────┼─────────────────┼──────────────┤
│   LogisticReg CV   │    0.97544     │ 0.98082  │ 0.98778 │     0.97814     │   0.98352    │
├────────────────────┼────────────────┼──────────┼─────────┼─────────────────┼──────────────┤
│       NuSVC        │    0.87719     │ 0.91139  │ 0.96325 │     0.84507     │   0.98901    │
├────────────────────┼────────────────┼──────────┼─────────┼─────────────────┼──────────────┤
│    RandomForest    │    0.97193     │  0.9779  │ 0.99277 │     0.98333     │   0.97253    │
├────────────────────┼────────────────┼──────────┼─────────┼─────────────────┼──────────────┤
│  RidgeClassifier   │     0.9614     │ 0.97003  │ 0.98389 │     0.96216     │   0.97802    │
├────────────────────┼────────────────┼──────────┼─────────┼─────────────────┼──────────────┤
│ RidgeClassifierCV  │    0.97544     │ 0.98082  │ 0.98778 │     0.97814     │   0.98352    │
├────────────────────┼────────────────┼──────────┼─────────┼─────────────────┼──────────────┤
│   SGDClassifier    │    0.81404     │  0.8729  │   N/A   │     0.77447     │     1.0      │
├────────────────────┼────────────────┼──────────┼─────────┼─────────────────┼──────────────┤
│        SVC         │     0.9193     │ 0.93931  │ 0.96954 │     0.90355     │   0.97802    │
├────────────────────┼────────────────┼──────────┼─────────┼─────────────────┼──────────────┤
│      XGBoost       │    0.97895     │ 0.98343  │ 0.99002 │     0.98889     │   0.97802    │
├────────────────────┼────────────────┼──────────┼─────────┼─────────────────┼──────────────┤
│      Lightgbm      │    0.97193     │ 0.97802  │ 0.98826 │     0.97802     │   0.97802    │
└────────────────────┴────────────────┴──────────┴─────────┴─────────────────┴──────────────┘

```
Plot

![Results](https://github.com/arnavrneo/LightPredict/blob/master/img/result.png)

### Roc-Auc Curves

LightClassifier can also plot the roc_auc curves directly. To plot them, use the following code:
```
lcf.roc_auc_curves(X_train,X_test,y_train,y_test)
```

Plot

![Curve](https://github.com/arnavrneo/LightPredict/blob/master/img/roc_auc.png)

### Optimization (using Optuna)
LightClassifier can also be used to tune for hyper-parameter tuning. It automatically tunes the hyper-parameters using Optuna and displays the best score of each model along with their best parameters. Optimization & Parameters importance plots can also be automatically generated using it. To do optimization, simply call:

```
lcf.optimize(X_train,X_test,y_train,y_test,trials=2) # Here, trials means no. of iterations and passing plot=True plot the graphs also.

 Optimizing models...
╔═══════════════════════════════════════════════════════════════════════════════════════════╗
║                                 Optimized Models & Scores                                 ║
╚═══════════════════════════════════════════════════════════════════════════════════════════╝

┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃       Model       ┃ Best Score ┃ Best Params                                              ┃
┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│     AdaBoost      │   0.972    │ {'n_estimators': 22, 'learning_rate':                    │
│                   │            │ 0.7865771576419738}                                      │
├───────────────────┼────────────┼──────────────────────────────────────────────────────────┤
│ BaggingClassifier │   0.979    │ {'n_estimators': 34, 'max_samples': 82}                  │
├───────────────────┼────────────┼──────────────────────────────────────────────────────────┤
│   Decision Tree   │   0.951    │ {'max_depth': 6, 'min_samples_split': 14,                │
│                   │            │ 'min_weight_fraction_leaf': 0.0359737959130258,          │
│                   │            │ 'min_samples_leaf': 10}                                  │
├───────────────────┼────────────┼──────────────────────────────────────────────────────────┤
│    Extra Trees    │   0.944    │ {'max_depth': 3}                                         │
├───────────────────┼────────────┼──────────────────────────────────────────────────────────┤
│    KNeighbors     │   0.947    │ {'n_neighbors': 10}                                      │
├───────────────────┼────────────┼──────────────────────────────────────────────────────────┤
│   RandomForest    │   0.972    │ {'max_depth': 4, 'n_estimators': 128,                    │
│                   │            │ 'min_samples_leaf': 1}                                   │
├───────────────────┼────────────┼──────────────────────────────────────────────────────────┤
│   XGBClassifier   │   0.982    │ {'max_depth': 6, 'n_estimators': 101, 'learning_rate':   │
│                   │            │ 0.5412564423401603, 'gamma': 0.20344958660655896,        │
│                   │            │ 'subsample': 0.5885059262168062}                         │
├───────────────────┼────────────┼──────────────────────────────────────────────────────────┤
│     LightGBM      │   0.972    │ {'lambda_l1': 0.07859371738975324, 'lambda_l2':          │
│                   │            │ 4.299498906836946, 'num_leaves': 100,                    │
│                   │            │ 'feature_fraction': 0.9307795680365831,                  │
│                   │            │ 'bagging_fraction': 0.5941966727501499, 'bagging_freq':  │
│                   │            │ 7, 'min_child_samples': 39}                              │
└───────────────────┴────────────┴──────────────────────────────────────────────────────────┘
```
